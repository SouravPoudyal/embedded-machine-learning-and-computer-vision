{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2308ac8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "626f97ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset location\n",
    "DATASET_PATH = \"Datasets/electronic-components-png\"\n",
    "\n",
    "# resolution of images\n",
    "TARGET_WIDTH = 28\n",
    "TARGET_HEIGHT = 28\n",
    "\n",
    "# invert image\n",
    "INVERT = False\n",
    "\n",
    "# Grayscale\n",
    "GRAYSCALE = True\n",
    "\n",
    "# 20%-validation, 20%-test\n",
    "VAL_RATIO = 0.2\n",
    "TEST_RATIO = 0.2\n",
    "TRAIN_RATIO = 1 - VAL_RATIO - TEST_RATIO \n",
    "\n",
    "# Normalization mean\n",
    "MEAN = (0.0864, 0.3011, 0.6495)\n",
    "STD = (1.212, 1.425, 1.505)\n",
    "\n",
    "GRAY_MEAN = 0.5\n",
    "GRAY_STD = 0.5\n",
    "\n",
    "# Hyperparameters\n",
    "LR = 0.001\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cf2ac15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare dataset\n",
    "def load_dataset(path, grayscale=True, invert=False):\n",
    "    data = []\n",
    "    class_names = []\n",
    "    class_map = {}\n",
    "    \n",
    "    # Walk through the dataset directory to collect images and labels\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        # Skip the root directory itself (only process subdirectories)\n",
    "        if root == path:\n",
    "            continue\n",
    "        \n",
    "        # Get label from directory name\n",
    "        label = os.path.basename(root)\n",
    "        \n",
    "        # Add label to class_map if not already present\n",
    "        if label not in class_map:\n",
    "            class_map[label] = len(class_map)\n",
    "            class_names.append(label)\n",
    "        \n",
    "        # Process each image file\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                img_path = os.path.join(root, file)\n",
    "                data.append((img_path, class_map[label]))\n",
    "    \n",
    "    return data, class_names, class_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14a96ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 unique classes:\n",
      "0: background (Count: 50)\n",
      "1: capacitor (Count: 50)\n",
      "2: diode (Count: 50)\n",
      "3: led (Count: 50)\n",
      "4: resistor (Count: 50)\n"
     ]
    }
   ],
   "source": [
    "# Load all data\n",
    "all_data, class_names, class_map = load_dataset(DATASET_PATH, grayscale=GRAYSCALE, invert=INVERT)\n",
    "\n",
    "# Print class information\n",
    "print(f\"Found {len(class_names)} unique classes:\")\n",
    "for i, label in enumerate(sorted(class_names)):\n",
    "    count = sum(1 for _, class_id in all_data if class_id == i)\n",
    "    print(f\"{i}: {label} (Count: {count})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf70ca24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "train_val_data, test_data = train_test_split(all_data, test_size=TEST_RATIO, random_state=42)\n",
    "train_data, val_data = train_test_split(train_val_data, test_size=VAL_RATIO/(1-TEST_RATIO), random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae929e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation and preprocessing\n",
    "def preprocess_image(image_path, label, grayscale=GRAYSCALE, invert=INVERT):\n",
    "    # Read image file\n",
    "    image = tf.io.read_file(image_path)\n",
    "    # Decode image\n",
    "    image = tf.image.decode_image(image, channels=3, expand_animations=False)\n",
    "    # Resize\n",
    "    image = tf.image.resize(image, [TARGET_HEIGHT, TARGET_WIDTH])\n",
    "    \n",
    "    if grayscale:\n",
    "        # Convert to grayscale\n",
    "        image = tf.image.rgb_to_grayscale(image)\n",
    "        # Normalize\n",
    "        image = (image / 255.0 - GRAY_MEAN) / GRAY_STD\n",
    "    else:\n",
    "        # Normalize RGB\n",
    "        image = (image / 255.0 - MEAN) / STD\n",
    "    \n",
    "    if invert:\n",
    "        image = 1.0 - image\n",
    "    \n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dab0cfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorFlow datasets\n",
    "def create_dataset(data, batch_size=BATCH_SIZE, shuffle=False):\n",
    "    image_paths = [item[0] for item in data]\n",
    "    labels = [item[1] for item in data]\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "    dataset = dataset.map(lambda x, y: preprocess_image(x, y, GRAYSCALE, INVERT))\n",
    "    \n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(data))\n",
    "    \n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abcbb647",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = create_dataset(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataset = create_dataset(val_data, batch_size=BATCH_SIZE)\n",
    "test_dataset = create_dataset(test_data, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0d92d35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize a sample\n",
    "def visualize_sample(dataset):\n",
    "    for images, labels in dataset.take(1):\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        image = images[0].numpy()\n",
    "        if GRAYSCALE:\n",
    "            # Denormalize\n",
    "            image = (image * GRAY_STD + GRAY_MEAN) * 255\n",
    "            image = np.clip(image, 0, 255).astype('uint8')\n",
    "            plt.imshow(image.squeeze(), cmap='gray')\n",
    "        else:\n",
    "            # Denormalize RGB\n",
    "            image = (image * STD + MEAN) * 255\n",
    "            image = np.clip(image, 0, 255).astype('uint8')\n",
    "            plt.imshow(image)\n",
    "        plt.title(f\"Label: {class_names[labels[0]]}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "visualize_sample(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2786a8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "def create_model(input_shape=(28, 28, 1), num_classes=5):\n",
    "    model = keras.Sequential([\n",
    "        layers.Conv2D(32, kernel_size=3, padding='same', activation='relu', input_shape=input_shape),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(pool_size=2),\n",
    "        layers.Dropout(0.4),\n",
    "        \n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbaccd9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               802944    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 804,037\n",
      "Trainable params: 803,973\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "if GRAYSCALE:\n",
    "    model = create_model(input_shape=(28, 28, 1), num_classes=len(class_names))\n",
    "else:\n",
    "    model = create_model(input_shape=(28, 28, 3), num_classes=len(class_names))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=LR),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b409ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "15/15 [==============================] - 6s 27ms/step - loss: 1.8265 - accuracy: 0.6133 - val_loss: 1.2839 - val_accuracy: 0.7200\n",
      "Epoch 2/10\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.4945 - accuracy: 0.8467 - val_loss: 1.2675 - val_accuracy: 0.5400\n",
      "Epoch 3/10\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.5319 - accuracy: 0.8600 - val_loss: 1.2209 - val_accuracy: 0.8400\n",
      "Epoch 4/10\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0870 - accuracy: 0.9733 - val_loss: 1.1715 - val_accuracy: 0.9000\n",
      "Epoch 5/10\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.1016 - accuracy: 0.9800 - val_loss: 1.1112 - val_accuracy: 0.9200\n",
      "Epoch 6/10\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0883 - accuracy: 0.9667 - val_loss: 1.0642 - val_accuracy: 0.9400\n",
      "Epoch 7/10\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 0.0190 - accuracy: 1.0000 - val_loss: 1.0013 - val_accuracy: 0.9200\n",
      "Epoch 8/10\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0873 - accuracy: 0.9733 - val_loss: 0.9757 - val_accuracy: 0.9400\n",
      "Epoch 9/10\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 0.0213 - accuracy: 0.9933 - val_loss: 0.9286 - val_accuracy: 0.9600\n",
      "Epoch 10/10\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 0.0348 - accuracy: 0.9933 - val_loss: 0.8763 - val_accuracy: 0.9800\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.8588 - accuracy: 0.9400\n",
      "Test accuracy: 0.9400\n"
     ]
    }
   ],
   "source": [
    "# Callbacks\n",
    "callbacks = [\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1),\n",
    "    keras.callbacks.ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss'),\n",
    "    keras.callbacks.TensorBoard(log_dir=f'logs/{time.strftime(\"%Y%m%d-%H%M%S\")}', histogram_freq=1)\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "print(f\"Test accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bce260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model output (raw predictions):\n",
      "background     : 0.0476\n",
      "capacitor      : 0.1408\n",
      "diode          : 0.1093\n",
      "led            : 0.6149\n",
      "resistor       : 0.0873\n",
      "\n",
      "Prediction summary:\n",
      "Predicted label: 3 - led\n",
      "Actual label:    3 - led\n",
      "Confidence:      61.5%\n",
      "\n",
      "✅ Correct prediction!\n"
     ]
    }
   ],
   "source": [
    "# Get one sample from validation set\n",
    "for sample_image, sample_label in val_dataset.unbatch().skip(20).take(1):\n",
    "    break\n",
    "\n",
    "# Prepare input (add batch dimension)\n",
    "input_image = np.expand_dims(sample_image, axis=0)\n",
    "\n",
    "# Make prediction\n",
    "y_pred = model.predict(input_image)\n",
    "predicted_label = np.argmax(y_pred, axis=1)[0]\n",
    "actual_label = sample_label.numpy()\n",
    "\n",
    "# Get class names (from your earlier class_map)\n",
    "labels = sorted(class_map.keys())  # List of class names in order\n",
    "\n",
    "# Visualize the sample\n",
    "plt.figure(figsize=(8, 4))\n",
    "if GRAYSCALE:\n",
    "    # Denormalize grayscale image\n",
    "    denormalized = (sample_image * GRAY_STD + GRAY_MEAN).numpy().squeeze()\n",
    "    plt.imshow(denormalized, cmap='gray', vmin=0, vmax=1)\n",
    "else:\n",
    "    # Denormalize RGB image\n",
    "    denormalized = (sample_image * STD + MEAN).numpy()\n",
    "    denormalized = np.clip(denormalized, 0, 1)\n",
    "    if denormalized.shape[0] == 3:  # CHW to HWC\n",
    "        denormalized = denormalized.transpose(1, 2, 0)\n",
    "    plt.imshow(denormalized)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Display prediction info\n",
    "print(\"\\nModel output (raw predictions):\")\n",
    "for i, score in enumerate(y_pred[0]):\n",
    "    print(f\"{labels[i]:<15}: {score:.4f}\")\n",
    "\n",
    "print(\"\\nPrediction summary:\")\n",
    "print(f\"Predicted label: {predicted_label} - {labels[predicted_label]}\")\n",
    "print(f\"Actual label:    {actual_label} - {labels[actual_label]}\")\n",
    "print(f\"Confidence:      {np.max(y_pred)*100:.1f}%\")\n",
    "\n",
    "# Show whether prediction was correct\n",
    "if predicted_label == actual_label:\n",
    "    print(\"\\n✅ Correct prediction!\")\n",
    "else:\n",
    "    print(\"\\n❌ Incorrect prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0c40cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
