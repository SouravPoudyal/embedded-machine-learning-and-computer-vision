{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cc8cd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c42e728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "\n",
    "# Dataset location\n",
    "DATASET_PATH = \"Datasets/electronic-components-png\"\n",
    "\n",
    "# resolution of images\n",
    "TARGET_WIDTH = 28\n",
    "TARGET_HEIGHT = 28\n",
    "\n",
    "# invert image\n",
    "INVERT = False\n",
    "\n",
    "#Grayscale\n",
    "GRAYSCALE = True\n",
    "\n",
    "# 20%-validation, 20%-test\n",
    "VAL_RATIO = 0.2\n",
    "TEST_RATIO = 0.2\n",
    "TRAIN_RATIO = 1 - VAL_RATIO - TEST_RATIO \n",
    "\n",
    "#Normalization mean\n",
    "MEAN = (0.0864, 0.3011, 0.6495)\n",
    "STD  =  (1.212, 1.425, 1.505)\n",
    "\n",
    "GRAY_MEAN = (0.5)\n",
    "GRAY_STD = (0.5)\n",
    "\n",
    "# Hyperparameters\n",
    "LR = 0.001\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f1e8334",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, path, mean, std, gray_mean, gray_std, train=True, invert=False, grayscale = True):\n",
    "        self.path = path\n",
    "        self.train = train\n",
    "        self.invert = invert\n",
    "        self.grayscale = grayscale\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((28,28)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(gray_mean, gray_std) if self.grayscale \n",
    "            else transforms.Normalize(mean, std)\n",
    "        ])\n",
    "        \n",
    "        # Initialize data storage\n",
    "        self.data = []\n",
    "        \n",
    "        # Build class map dynamically from folder names\n",
    "        self.class_map = {}\n",
    "        \n",
    "        if self.train:\n",
    "            # Walk through the dataset directory to collect images and labels\n",
    "            for root, dirs, files in os.walk(self.path):\n",
    "                # Skip the root directory itself (only process subdirectories)\n",
    "                if root == self.path:\n",
    "                    continue\n",
    "                \n",
    "                # Get label from directory name\n",
    "                label = os.path.basename(root)\n",
    "                \n",
    "                # Add label to class_map if not already present\n",
    "                if label not in self.class_map:\n",
    "                    self.class_map[label] = len(self.class_map)\n",
    "                \n",
    "                # Process each image file\n",
    "                for file in files:\n",
    "                    if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                        img_path = os.path.join(root, file)\n",
    "                        self.data.append((label, img_path))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        class_name, img_path = self.data[index]\n",
    "        \n",
    "        # Load and transform image\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        if self.grayscale and image.shape[0] == 3:  # RGB\n",
    "            image = transforms.functional.rgb_to_grayscale(image)\n",
    "        if self.invert:\n",
    "            image = 255 - image\n",
    "        # Get class ID from mapping\n",
    "        class_id = self.class_map[class_name]\n",
    "        \n",
    "        return image, torch.tensor(class_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ae30a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 unique classes:\n",
      "0: background (Count: 50)\n",
      "1: capacitor (Count: 50)\n",
      "2: diode (Count: 50)\n",
      "3: led (Count: 50)\n",
      "4: resistor (Count: 50)\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset = CustomDataset(DATASET_PATH, MEAN, STD, GRAY_MEAN, GRAY_STD, grayscale = GRAYSCALE)\n",
    "\n",
    "# Print unique labels and their counts\n",
    "unique_labels = set()\n",
    "for label, _ in dataset.data:\n",
    "    unique_labels.add(label)\n",
    "\n",
    "print(f\"Found {len(unique_labels)} unique classes:\")\n",
    "for i, label in enumerate(sorted(unique_labels)):\n",
    "    print(f\"{i}: {label} (Count: {sum(1 for l, _ in dataset.data if l == label)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b23760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dividing into train, test and validation set\n",
    "num_dataset = len(dataset.data)\n",
    "num_test = int(TEST_RATIO * num_dataset)\n",
    "num_val = int(VAL_RATIO * num_dataset)\n",
    "num_train = int(TRAIN_RATIO * num_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec8b3a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    dataset,\n",
    "    [num_train, num_val, num_test],\n",
    "    generator=torch.Generator().manual_seed(42)  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "40602ebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = 100\n",
    "\n",
    "image, lable = train_dataset[idx]\n",
    "\n",
    "if GRAYSCALE:\n",
    "    # --- DENORMALIZE ---\n",
    "    denormalized = image * GRAY_STD + GRAY_MEAN\n",
    "    # Remove channel dim if needed\n",
    "    if denormalized.dim() == 3:\n",
    "        denormalized = denormalized.squeeze(0)  # Now [H, W]\n",
    "    # Convert to numpy and scale to 0-255\n",
    "    display_image = (denormalized * 255).numpy().astype('uint8')\n",
    "    # --- SHOW IMAGE ---\n",
    "    plt.imshow(display_image, cmap='gray', vmin=0, vmax=255)\n",
    "else:\n",
    "    # --- DENORMALIZE RGB ---\n",
    "    denormalized = image * torch.tensor(STD)[:, None, None] + torch.tensor(MEAN)[:, None, None]\n",
    "    # Convert to displayable format (HWC for matplotlib)\n",
    "    display_show = denormalized.permute(1, 2, 0).numpy()  # [H, W, C]\n",
    "    # Clip values to [0,1] range first\n",
    "    display_show = np.clip(display_show, 0, 1)\n",
    "    plt.imshow(display_show)\n",
    "\n",
    "# --- SHOW IMAGE ---\n",
    "plt.title(f\"Label: {label}\")\n",
    "plt.axis('on')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e9fb2299",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, in_channels=1, num_classes=5):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        \n",
    "        # Single convolution block\n",
    "        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=3, padding=1)  # Output: [32, 28, 28]\n",
    "        self.bn1 = nn.BatchNorm2d(32)  # Batch norm for conv output\n",
    "        self.pool = nn.MaxPool2d(2, 2)  # Reduces to [32, 14, 14]\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        \n",
    "        # Calculate flattened size\n",
    "        self.flattened_size = 32 * 14 * 14  # After pooling\n",
    "        \n",
    "        # Classifier\n",
    "        self.fc1 = nn.Linear(self.flattened_size, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Conv block\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Classifier\n",
    "        x = x.view(-1, self.flattened_size)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)  # Dropout before final layer\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    def save_model(self):\n",
    "        \n",
    "        #############################\n",
    "        # Saving the model's weitghts\n",
    "        # Upload 'model' as part of\n",
    "        # your submission\n",
    "        # Do not modify this function\n",
    "        #############################\n",
    "        \n",
    "        torch.save(self.state_dict(), 'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce04f72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bd13b803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 28, 28]             320\n",
      "       BatchNorm2d-2           [-1, 32, 28, 28]              64\n",
      "         MaxPool2d-3           [-1, 32, 14, 14]               0\n",
      "           Dropout-4           [-1, 32, 14, 14]               0\n",
      "            Linear-5                  [-1, 128]         802,944\n",
      "           Dropout-6                  [-1, 128]               0\n",
      "            Linear-7                    [-1, 5]             645\n",
      "================================================================\n",
      "Total params: 803,973\n",
      "Trainable params: 803,973\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.48\n",
      "Params size (MB): 3.07\n",
      "Estimated Total Size (MB): 3.55\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SimpleCNN(in_channels=1, num_classes=5).to(device)  # Your model\n",
    "summary(model, input_size=(1, 28, 28))  # Grayscale input (1 channel, 28x28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "765e66c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: Train Loss: 1.0404 | Train Acc: 0.5733 | Val Loss: 0.8390 | Val Acc: 0.7600 | LR: 0.001000\n",
      "Epoch 2/10: Train Loss: 0.4229 | Train Acc: 0.8467 | Val Loss: 0.3761 | Val Acc: 0.8800 | LR: 0.001000\n",
      "Epoch 3/10: Train Loss: 0.2904 | Train Acc: 0.9133 | Val Loss: 0.3155 | Val Acc: 0.8800 | LR: 0.001000\n",
      "Epoch 4/10: Train Loss: 0.1887 | Train Acc: 0.9267 | Val Loss: 0.3595 | Val Acc: 0.8200 | LR: 0.001000\n",
      "Epoch 5/10: Train Loss: 0.1223 | Train Acc: 0.9533 | Val Loss: 0.2120 | Val Acc: 0.9200 | LR: 0.001000\n",
      "Epoch 6/10: Train Loss: 0.1503 | Train Acc: 0.9533 | Val Loss: 0.3268 | Val Acc: 0.8600 | LR: 0.001000\n",
      "Epoch 7/10: Train Loss: 0.1745 | Train Acc: 0.9333 | Val Loss: 0.2103 | Val Acc: 0.9200 | LR: 0.001000\n",
      "Epoch 8/10: Train Loss: 0.0965 | Train Acc: 0.9733 | Val Loss: 0.3467 | Val Acc: 0.8800 | LR: 0.001000\n",
      "Epoch 9/10: Train Loss: 0.1126 | Train Acc: 0.9600 | Val Loss: 0.3266 | Val Acc: 0.8800 | LR: 0.001000\n",
      "Epoch 10/10: Train Loss: 0.1180 | Train Acc: 0.9333 | Val Loss: 0.2111 | Val Acc: 0.9000 | LR: 0.000500\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import time\n",
    "\n",
    "# Initialize TensorBoard\n",
    "writer = SummaryWriter(f'runs/{time.strftime(\"%Y%m%d-%H%M%S\")}')\n",
    "\n",
    "# Model, optimizer and scheduler\n",
    "model = SimpleCNN(in_channels=1, num_classes=5)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.5)\n",
    "\n",
    "# Data\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)  # Need validation set\n",
    "\n",
    "def get_num_correct(preds, labels):\n",
    "    return preds.argmax(dim=1).eq(labels).sum().item()\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        images, labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        preds = model(images)\n",
    "        loss = F.cross_entropy(preds, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        train_correct += get_num_correct(preds, labels)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            images, labels = batch\n",
    "            preds = model(images)\n",
    "            loss = F.cross_entropy(preds, labels)\n",
    "            \n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            val_correct += get_num_correct(preds, labels)\n",
    "    \n",
    "    # Metrics\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_acc = train_correct / len(train_loader.dataset)\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_acc = val_correct / len(val_loader.dataset)\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # TensorBoard logging\n",
    "    writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/train', train_acc, epoch)\n",
    "    writer.add_scalar('Loss/val', val_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/val', val_acc, epoch)\n",
    "    writer.add_scalar('Learning Rate', optimizer.param_groups[0]['lr'], epoch)\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    \n",
    "    # Print stats\n",
    "    print(f'Epoch {epoch+1}/{EPOCHS}: '\n",
    "          f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | '\n",
    "          f'Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | '\n",
    "          f'LR: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cc698cdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[[0.2051, 0.2097, 0.2097,  ..., 0.2336, 0.2363, 0.2354],\n",
       "           [0.1981, 0.2143, 0.2143,  ..., 0.2451, 0.2585, 0.2622],\n",
       "           [0.2097, 0.2143, 0.2235,  ..., 0.2716, 0.2678, 0.2493],\n",
       "           ...,\n",
       "           [0.1516, 0.1516, 0.1631,  ..., 0.2757, 0.2803, 0.2803],\n",
       "           [0.1294, 0.1405, 0.1516,  ..., 0.2470, 0.2692, 0.2692],\n",
       "           [0.1072, 0.1216, 0.1516,  ..., 0.2327, 0.2406, 0.2360]]],\n",
       " \n",
       " \n",
       "         [[[0.0888, 0.0888, 0.0980,  ..., 0.1516, 0.1516, 0.1516],\n",
       "           [0.0980, 0.1026, 0.1026,  ..., 0.1516, 0.1516, 0.1516],\n",
       "           [0.1026, 0.1026, 0.1072,  ..., 0.1516, 0.1516, 0.1516],\n",
       "           ...,\n",
       "           [0.1461, 0.1424, 0.1470,  ..., 0.1608, 0.1608, 0.1654],\n",
       "           [0.1428, 0.1516, 0.1516,  ..., 0.1608, 0.1654, 0.1608],\n",
       "           [0.1415, 0.1470, 0.1516,  ..., 0.1516, 0.1608, 0.1654]]],\n",
       " \n",
       " \n",
       "         [[[0.1571, 0.1700, 0.1663,  ..., 0.1930, 0.1768, 0.1865],\n",
       "           [0.1608, 0.1700, 0.1709,  ..., 0.1911, 0.1795, 0.1818],\n",
       "           [0.1608, 0.1654, 0.1718,  ..., 0.1958, 0.1958, 0.1865],\n",
       "           ...,\n",
       "           [0.1183, 0.1174, 0.1105,  ..., 0.2097, 0.2143, 0.2097],\n",
       "           [0.1026, 0.1026, 0.1026,  ..., 0.1867, 0.1875, 0.1824],\n",
       "           [0.0888, 0.0888, 0.1026,  ..., 0.1723, 0.1700, 0.1700]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[0.1732, 0.2032, 0.2143,  ..., 0.2281, 0.2143, 0.2143],\n",
       "           [0.1700, 0.2000, 0.2143,  ..., 0.2327, 0.2281, 0.2189],\n",
       "           [0.1811, 0.2111, 0.2143,  ..., 0.2327, 0.2327, 0.2327],\n",
       "           ...,\n",
       "           [0.0888, 0.0980, 0.1072,  ..., 0.2803, 0.2803, 0.2803],\n",
       "           [0.0783, 0.0870, 0.0861,  ..., 0.2803, 0.2849, 0.2849],\n",
       "           [0.0515, 0.0585, 0.0539,  ..., 0.2803, 0.2803, 0.2803]]],\n",
       " \n",
       " \n",
       "         [[[0.1588, 0.1606, 0.1624,  ..., 0.2270, 0.2223, 0.2223],\n",
       "           [0.1681, 0.1670, 0.1716,  ..., 0.2353, 0.2335, 0.2307],\n",
       "           [0.1716, 0.1771, 0.1771,  ..., 0.2399, 0.2353, 0.2307],\n",
       "           ...,\n",
       "           [0.1725, 0.1771, 0.1795,  ..., 0.2586, 0.2563, 0.2460],\n",
       "           [0.1670, 0.1762, 0.1809,  ..., 0.2266, 0.2330, 0.2335],\n",
       "           [0.1580, 0.1597, 0.1679,  ..., 0.2179, 0.2206, 0.2215]]],\n",
       " \n",
       " \n",
       "         [[[0.1516, 0.1516, 0.1516,  ..., 0.1770, 0.1723, 0.1700],\n",
       "           [0.1516, 0.1562, 0.1608,  ..., 0.1747, 0.1723, 0.1700],\n",
       "           [0.1608, 0.1654, 0.1654,  ..., 0.1747, 0.1700, 0.1700],\n",
       "           ...,\n",
       "           [0.1921, 0.2056, 0.2143,  ..., 0.2327, 0.2327, 0.2327],\n",
       "           [0.1834, 0.1857, 0.2065,  ..., 0.2281, 0.2327, 0.2327],\n",
       "           [0.1700, 0.1732, 0.2032,  ..., 0.2281, 0.2327, 0.2327]]]]),\n",
       " tensor([1, 0, 2, 3, 0, 3, 2, 4, 3, 4])]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Inference\n",
    "\n",
    "idx = 0 \n",
    "val_data = next(iter(val_loader))\n",
    "val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9180c0b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
