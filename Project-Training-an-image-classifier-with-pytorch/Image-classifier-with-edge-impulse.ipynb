{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2308ac8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    " \n",
    "import threading, queue, time, json, hmac, hashlib,requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b4ddd405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "\n",
    "# Edge Impule > Keys\n",
    "EI_API_KEY = \"ei_5bbb1e6dfc726e61d41451ee2ea8422fa592dade9d9eae201edca48750746761\"\n",
    "EI_HMAC_KEY = \"4269d8e86796d0e6a4a46af7aef67062\"\n",
    "\n",
    "# Number of threads to run to upload data to Edge Impulse\n",
    "NUM_THREADS = 20\n",
    "\n",
    "# Dataset location\n",
    "DATASET_PATH = \"Datasets/electronic-components-png\"\n",
    "\n",
    "# resolution of images\n",
    "TARGET_WIDTH = 28\n",
    "TARGET_HEIGHT = 28\n",
    "\n",
    "# invert image\n",
    "INVERT = False\n",
    "\n",
    "# Grayscale\n",
    "GRAYSCALE = True\n",
    "\n",
    "# 20%-validation, 20%-test\n",
    "VAL_RATIO = 0.2\n",
    "TEST_RATIO = 0.2\n",
    "TRAIN_RATIO = 1 - VAL_RATIO - TEST_RATIO \n",
    "\n",
    "# to Use CNN\n",
    "FLATTEN = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "626f97ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization mean\n",
    "MEAN = (0.0864, 0.3011, 0.6495)\n",
    "STD = (1.212, 1.425, 1.505)\n",
    "\n",
    "GRAY_MEAN = 0.5\n",
    "GRAY_STD = 0.5\n",
    "\n",
    "# Hyperparameters\n",
    "LR = 0.001\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2cf2ac15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare dataset\n",
    "def load_dataset(path, grayscale=True, invert=False):\n",
    "    data = []\n",
    "    class_names = []\n",
    "    class_map = {}\n",
    "    \n",
    "    # Walk through the dataset directory to collect images and labels\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        # Skip the root directory itself (only process subdirectories)\n",
    "        if root == path:\n",
    "            continue\n",
    "        \n",
    "        # Get label from directory name\n",
    "        label = os.path.basename(root)\n",
    "        \n",
    "        # Add label to class_map if not already present\n",
    "        if label not in class_map:\n",
    "            class_map[label] = len(class_map)\n",
    "            class_names.append(label)\n",
    "        \n",
    "        # Process each image file\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                img_path = os.path.join(root, file)\n",
    "                data.append((img_path, class_map[label]))\n",
    "    \n",
    "    return data, class_names, class_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "14a96ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 unique classes:\n",
      "0: background (Count: 50)\n",
      "1: capacitor (Count: 50)\n",
      "2: diode (Count: 50)\n",
      "3: led (Count: 50)\n",
      "4: resistor (Count: 50)\n"
     ]
    }
   ],
   "source": [
    "# Load all data\n",
    "all_data, class_names, class_map = load_dataset(DATASET_PATH, grayscale=GRAYSCALE, invert=INVERT)\n",
    "\n",
    "# Print class information\n",
    "print(f\"Found {len(class_names)} unique classes:\")\n",
    "for i, label in enumerate(sorted(class_names)):\n",
    "    count = sum(1 for _, class_id in all_data if class_id == i)\n",
    "    print(f\"{i}: {label} (Count: {count})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cf70ca24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "train_val_data, test_data = train_test_split(all_data, test_size=TEST_RATIO, random_state=42)\n",
    "train_data, val_data = train_test_split(train_val_data, test_size=VAL_RATIO/(1-TEST_RATIO), random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ae929e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation and preprocessing\n",
    "def preprocess_image(image_path, label, grayscale=GRAYSCALE, invert=INVERT):\n",
    "    # Read image file\n",
    "    image = tf.io.read_file(image_path)\n",
    "    # Decode image\n",
    "    image = tf.image.decode_image(image, channels=3, expand_animations=False)\n",
    "    # Resize\n",
    "    image = tf.image.resize(image, [TARGET_HEIGHT, TARGET_WIDTH])\n",
    "    \n",
    "    if grayscale:\n",
    "        # Convert to grayscale\n",
    "        image = tf.image.rgb_to_grayscale(image)\n",
    "        # Normalize\n",
    "        image = (image / 255.0 - GRAY_MEAN) / GRAY_STD\n",
    "    else:\n",
    "        # Normalize RGB\n",
    "        image = (image / 255.0 - MEAN) / STD\n",
    "    \n",
    "    if invert:\n",
    "        image = 1.0 - image\n",
    "    \n",
    "    # Flatten the image (for DNN)\n",
    "    if FLATTEN:\n",
    "        # Calculate total elements (H * W * C)\n",
    "        if grayscale:\n",
    "            len_vector = TARGET_HEIGHT * TARGET_WIDTH * 1  # 1 channel\n",
    "        else:\n",
    "            len_vector = TARGET_HEIGHT * TARGET_WIDTH * 3  # 3 channels (RGB)\n",
    "        image = tf.reshape(image, [len_vector])  # Flatten to 1D)\n",
    "    \n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dab0cfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorFlow datasets\n",
    "def create_dataset(data, batch_size=BATCH_SIZE, shuffle=False):\n",
    "    image_paths = [item[0] for item in data]\n",
    "    labels = [item[1] for item in data]\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "    dataset = dataset.map(lambda x, y: preprocess_image(x, y, GRAYSCALE, INVERT))\n",
    "    \n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(data))\n",
    "    \n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "abcbb647",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = create_dataset(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataset = create_dataset(val_data, batch_size=BATCH_SIZE)\n",
    "test_dataset = create_dataset(test_data, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a0d92d35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize a sample\n",
    "def visualize_sample(dataset):\n",
    "    for images, labels in dataset.take(1):\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        image = images[0].numpy()\n",
    "        if GRAYSCALE:\n",
    "            # Denormalize\n",
    "            image = (image * GRAY_STD + GRAY_MEAN) * 255\n",
    "            image = np.clip(image, 0, 255).astype('uint8')\n",
    "            plt.imshow(image.squeeze(), cmap='gray')\n",
    "        else:\n",
    "            # Denormalize RGB\n",
    "            image = (image * STD + MEAN) * 255\n",
    "            image = np.clip(image, 0, 255).astype('uint8')\n",
    "            plt.imshow(image)\n",
    "        plt.title(f\"Label: {class_names[labels[0]]}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "visualize_sample(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f539bf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "FLATTEN = True\n",
    "TRAIN_SIZE = 200\n",
    "TEST_SIZE = 50\n",
    "# Create flattened datasets for DNN\n",
    "train_dataset_flat = create_dataset(train_val_data, batch_size=TRAIN_SIZE)\n",
    "test_dataset_flat = create_dataset(test_data, batch_size=TEST_SIZE)\n",
    "\n",
    "def extract_samples_and_labels(dataset):\n",
    "    X = []\n",
    "    y = []\n",
    "    for samples, labels in dataset:\n",
    "        X.extend(samples.numpy())\n",
    "        y.extend(labels.numpy())\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d3923c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_json_wrapper():\n",
    "  \"\"\"\n",
    "  Construct initial JSON wrapper as a template\n",
    "  \"\"\"\n",
    "\n",
    "  # Start with all zeros. Hs256 gives 32 bytes and we encode in hex. So, we need 64 characters here.\n",
    "  empty_signature = ''.join(['0'] * 64)\n",
    "\n",
    "  # Create JSON wrapper for data\n",
    "  data = {\n",
    "      \"protected\": {\n",
    "          \"ver\": \"v1\",\n",
    "          \"alg\": \"HS256\",\n",
    "          \"iat\": time.time()                  # Epoch time, seconds since 1970\n",
    "      },\n",
    "      \"signature\": empty_signature,\n",
    "      \"payload\": {\n",
    "          \"device_type\": \"pre-made\",          # Pre-made dataset (not collected)\n",
    "          \"interval_ms\": 1,                   # Pretend it's interval of 1 ms\n",
    "          \"sensors\": [\n",
    "              { \"name\": \"img\", \"units\": \"B\" } # Unitless (\"Byte\" data)\n",
    "          ],\n",
    "          \"values\": []\n",
    "      }\n",
    "  }\n",
    "\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3d38f515",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_sample(data, label, test_set=False):\n",
    "  \"\"\"\n",
    "  Send raw data sample to Edge Impulse project, return HTTP status code\n",
    "  \"\"\"\n",
    "\n",
    "  # Encode message in JSON format\n",
    "  encoded = json.dumps(data)\n",
    "\n",
    "  # Sign message\n",
    "  signature = hmac.new(bytes(EI_HMAC_KEY, 'utf-8'), \n",
    "                      msg = encoded.encode('utf-8'), \n",
    "                      digestmod = hashlib.sha256).hexdigest()\n",
    "\n",
    "  # Set the signature in the message and encode data again to JSON format\n",
    "  data['signature'] = signature\n",
    "  encoded = json.dumps(data)\n",
    "\n",
    "  # Construct URL based on dataset being sent\n",
    "  if test_set:\n",
    "    url = 'https://ingestion.edgeimpulse.com/api/testing/data'\n",
    "  else:\n",
    "    url = 'https://ingestion.edgeimpulse.com/api/training/data'\n",
    "\n",
    "  # Upload the data to project\n",
    "  res = requests.post(url=url,\n",
    "                      data=encoded,\n",
    "                      headers={\n",
    "                          'Content-Type': 'application/json',\n",
    "                          'x-file-name': str(label),\n",
    "                          'x-api-key': EI_API_KEY\n",
    "                      })\n",
    "  \n",
    "  return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "076101c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_sample(len_vector, is_test_set):\n",
    "  \"\"\"\n",
    "  Pull sample and label from queue and send to Edge Impulse server. To be called within a thread.\n",
    "  \"\"\"\n",
    "\n",
    "  global q\n",
    "\n",
    "  while not q.empty():\n",
    "\n",
    "    # Start with empty JSON wrapper\n",
    "    data = create_json_wrapper()\n",
    "\n",
    "    # Fill up values field (we need to convert to float to avoid JSON error)\n",
    "    sample, label = q.get()\n",
    "    for j in range(len_vector):\n",
    "      data['payload']['values'].append(float(sample[j]))\n",
    "\n",
    "    # Send sample\n",
    "    res = send_sample(data, label, test_set=is_test_set)\n",
    "\n",
    "    # Check response code\n",
    "    if (res.status_code != 200):\n",
    "      print(\"Failed to upload file to Edge Impulse\", res.status_code, res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b0167639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract training data\n",
    "X_train, y_train = extract_samples_and_labels(train_dataset_flat)\n",
    "num_samples_train = len(X_train)\n",
    "len_vector = X_train.shape[1]  # Assuming shape is (num_samples, feature_length)\n",
    "\n",
    "### Use many threads to send training data and labels to Edge Impulse project\n",
    "\n",
    "# Fill queue with training data and labels\n",
    "q = queue.Queue()\n",
    "for i in range(num_samples_train):\n",
    "  q.put((X_train[i], y_train[i]))\n",
    "\n",
    "# Create and start threads\n",
    "threads = []\n",
    "for i in range(NUM_THREADS):\n",
    "  threads.append(threading.Thread(target=upload_sample, args=(len_vector, False)))\n",
    "  threads[i].start()\n",
    "\n",
    "# Wait for threads to be done\n",
    "for thread in threads:\n",
    "  thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ce752545",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = extract_samples_and_labels(test_dataset_flat)\n",
    "num_samples_test = len(X_test)\n",
    "\n",
    "\n",
    "### Use many threads to send test data and labels to Edge Impulse project\n",
    "\n",
    "# Fill queue with test data and labels\n",
    "q = queue.Queue()\n",
    "for i in range(num_samples_test):\n",
    "  q.put((X_test[i], y_test[i]))\n",
    "\n",
    "# Create and start threads\n",
    "threads = []\n",
    "for i in range(NUM_THREADS):\n",
    "  threads.append(threading.Thread(target=upload_sample, args=(len_vector, True)))\n",
    "  threads[i].start()\n",
    "\n",
    "# Wait for threads to be done\n",
    "for thread in threads:\n",
    "  thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21ff3a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 784)\n"
     ]
    }
   ],
   "source": [
    "for image, lable in train_dataset_flat.take(1):\n",
    "    print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2786a8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "def create_model(input_shape=(28, 28, 1), num_classes=5):\n",
    "    model = keras.Sequential([\n",
    "        layers.Conv2D(32, kernel_size=3, padding='same', activation='relu', input_shape=input_shape),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(pool_size=2),\n",
    "        layers.Dropout(0.4),\n",
    "        \n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbaccd9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               802944    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 804,037\n",
      "Trainable params: 803,973\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "if GRAYSCALE:\n",
    "    model = create_model(input_shape=(28, 28, 1), num_classes=len(class_names))\n",
    "else:\n",
    "    model = create_model(input_shape=(28, 28, 3), num_classes=len(class_names))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=LR),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b409ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "15/15 [==============================] - 6s 27ms/step - loss: 1.8265 - accuracy: 0.6133 - val_loss: 1.2839 - val_accuracy: 0.7200\n",
      "Epoch 2/10\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.4945 - accuracy: 0.8467 - val_loss: 1.2675 - val_accuracy: 0.5400\n",
      "Epoch 3/10\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.5319 - accuracy: 0.8600 - val_loss: 1.2209 - val_accuracy: 0.8400\n",
      "Epoch 4/10\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0870 - accuracy: 0.9733 - val_loss: 1.1715 - val_accuracy: 0.9000\n",
      "Epoch 5/10\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.1016 - accuracy: 0.9800 - val_loss: 1.1112 - val_accuracy: 0.9200\n",
      "Epoch 6/10\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0883 - accuracy: 0.9667 - val_loss: 1.0642 - val_accuracy: 0.9400\n",
      "Epoch 7/10\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 0.0190 - accuracy: 1.0000 - val_loss: 1.0013 - val_accuracy: 0.9200\n",
      "Epoch 8/10\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0873 - accuracy: 0.9733 - val_loss: 0.9757 - val_accuracy: 0.9400\n",
      "Epoch 9/10\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 0.0213 - accuracy: 0.9933 - val_loss: 0.9286 - val_accuracy: 0.9600\n",
      "Epoch 10/10\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 0.0348 - accuracy: 0.9933 - val_loss: 0.8763 - val_accuracy: 0.9800\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.8588 - accuracy: 0.9400\n",
      "Test accuracy: 0.9400\n"
     ]
    }
   ],
   "source": [
    "# Callbacks\n",
    "callbacks = [\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1),\n",
    "    keras.callbacks.ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss'),\n",
    "    keras.callbacks.TensorBoard(log_dir=f'logs/{time.strftime(\"%Y%m%d-%H%M%S\")}', histogram_freq=1)\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "print(f\"Test accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bce260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model output (raw predictions):\n",
      "background     : 0.0476\n",
      "capacitor      : 0.1408\n",
      "diode          : 0.1093\n",
      "led            : 0.6149\n",
      "resistor       : 0.0873\n",
      "\n",
      "Prediction summary:\n",
      "Predicted label: 3 - led\n",
      "Actual label:    3 - led\n",
      "Confidence:      61.5%\n",
      "\n",
      "✅ Correct prediction!\n"
     ]
    }
   ],
   "source": [
    "# Get one sample from validation set\n",
    "for sample_image, sample_label in val_dataset.unbatch().skip(20).take(1):\n",
    "    break\n",
    "\n",
    "# Prepare input (add batch dimension)\n",
    "input_image = np.expand_dims(sample_image, axis=0)\n",
    "\n",
    "# Make prediction\n",
    "y_pred = model.predict(input_image)\n",
    "predicted_label = np.argmax(y_pred, axis=1)[0]\n",
    "actual_label = sample_label.numpy()\n",
    "\n",
    "# Get class names (from your earlier class_map)\n",
    "labels = sorted(class_map.keys())  # List of class names in order\n",
    "\n",
    "# Visualize the sample\n",
    "plt.figure(figsize=(8, 4))\n",
    "if GRAYSCALE:\n",
    "    # Denormalize grayscale image\n",
    "    denormalized = (sample_image * GRAY_STD + GRAY_MEAN).numpy().squeeze()\n",
    "    plt.imshow(denormalized, cmap='gray', vmin=0, vmax=1)\n",
    "else:\n",
    "    # Denormalize RGB image\n",
    "    denormalized = (sample_image * STD + MEAN).numpy()\n",
    "    denormalized = np.clip(denormalized, 0, 1)\n",
    "    if denormalized.shape[0] == 3:  # CHW to HWC\n",
    "        denormalized = denormalized.transpose(1, 2, 0)\n",
    "    plt.imshow(denormalized)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Display prediction info\n",
    "print(\"\\nModel output (raw predictions):\")\n",
    "for i, score in enumerate(y_pred[0]):\n",
    "    print(f\"{labels[i]:<15}: {score:.4f}\")\n",
    "\n",
    "print(\"\\nPrediction summary:\")\n",
    "print(f\"Predicted label: {predicted_label} - {labels[predicted_label]}\")\n",
    "print(f\"Actual label:    {actual_label} - {labels[actual_label]}\")\n",
    "print(f\"Confidence:      {np.max(y_pred)*100:.1f}%\")\n",
    "\n",
    "# Show whether prediction was correct\n",
    "if predicted_label == actual_label:\n",
    "    print(\"\\n✅ Correct prediction!\")\n",
    "else:\n",
    "    print(\"\\n❌ Incorrect prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0c40cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
